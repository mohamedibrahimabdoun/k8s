*  To replicate a set of Pods is to schedule a single Pod on every node within the cluster.
* Used to deploy system daemons such as log collectors and monitoring agents, which typically must run on every node.

DaemonSet Scheduler:
---------------------
* By default a DaemonSet will create a copy of a Pod on every node unless a node selector is used, which will limit eligible nodes to those with a matching set of labels.
* If a new node is added to the cluster, then the DaemonSet controller notices that	it is missing a Pod and adds the Pod to the new node.

Creating DaemonSets:
-------------------
* DaemonSets require a unique name across all DaemonSets in a given Kubernetes namespace


apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
 name: fluentd
 namespace: kube- system
 labels:
  app: fluentd
spec:
 template:
  metadata:
   labels:
    app: fluentd
  spec:
   containers:
    - name: fluentd
      image: fluent/fluentd: v0. 14. 10
      resources:
       limits:
        memory: 200Mi
       requests:
        cpu: 100m
        memory: 200Mi
      volumeMounts:
       - name: varlog
         mountPath: /var/log
       - name: varlibdockercontainers
         mountPath: /var/lib/docker/containers
         readOnly: true
  terminationGracePeriodSeconds: 30
  volumes:
    - name: varlog
      hostPath:
      path: /var/log
	- name: varlibdockercontainers
	  hostPath:
      path: /var/lib/docker/containers
	  
*  you can query its current state using the kubectl describe command:
	$ kubectl describe daemonset fluentd

Limiting DaemonSets to Specific Nodes:
======================================

Adding Labels to Nodes:
-----------------------
* The first step in limiting DaemonSets to specific nodes is to add the desired set of labels to a subset of nodes. This can be achieved using the kubectl label command.
$ kubectl label nodes k0-default-pool-35609c18-z7tb ssd=true

* To list only the nodes that have the ssd label set to true, use the kubectl get nodes command with the --selector flag:
	$ kubectl get nodes --selector ssd=true

Node Selectors:
---------------
* Node selectors can be used to limit what nodes a Pod can run on in a given Kubernetes cluster. Node selectors are defined as part of the Pod spec when creating a DaemonSet.

apiVersion: extensions/v1beta1
kind: "DaemonSet"
metadata:
 labels:
   app: nginx
   ssd: "true"
 name: nginx- fast- storage
spec:
 template:
  metadata:
   labels:
    app: nginx
    ssd: "true"
  spec:
   nodeSelector:
    ssd: "true"
   containers:
    - name: nginx
      image: nginx: 1. 10. 0
	
	  

Updating a DaemonSet:
=====================



Updating a DaemonSet by Deleting Individual Pods:
------------------------------------------------
* f you are running a pre-1.6 version of Kubernetes, you can perform a rolling delete of the Pods a DaemonSet manages using a for loop on your own machine to update one DaemonSet Pod every 60 seconds:

	PODS=$( kubectl get pods - o j sonpath - template=' {. items[*] . metadata. name}'
	for x in $PODS; do
		kubectl delete pods ${x}
		sleep 60
	done

Rolling Update of a DaemonSet:
------------------------------
* To set a DaemonSet to use the rolling update strategy, you need to configure the update strategy using the spec. updateStrategy. type field.
* That field should have the value RollingUpdate
*  There are two parameters that control the rolling update of a DaemonSet:
	# spec. minReadySeconds:  which determines how long a Pod must be “ready” before the rolling update proceeds to upgrade subsequent Pods.
	# spec. updateStrategy.rollingUpdate. maxUnavailable:  which indicates how many Pods may be simultaneously updated by the rolling update.
* A good approach might be to set maxUnavailable to 1 and only increase it if users or administrators complain about DaemonSet rollout speed.

	$ kubectl rollout status daemonSets my- daemon- set 

Deleting a DaemonSet:
======================

$ kubectl delete - f fluentd. yaml

* Deleting a DaemonSet will also delete all the Pods being managed by that DaemonSet. Set the --cascade flag to false to ensure only the DaemonSet is deleted and not the Pods.

